{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_data(filename):\n",
    "    \"\"\"Function to load the dataset\n",
    "    \n",
    "    Args:\n",
    "        -filename (str): Name of the dataset.\n",
    "        \n",
    "    Returns:\n",
    "        -data (dataframe): Dataset of the sampled transitions.\n",
    "        -states (list): List of states in the samples (subset of the full state space).\n",
    "        -actions (list): List of actions in the samples (subset of the full action space).\n",
    "        -discount (float): Discount factor.\n",
    "    \n",
    "    \"\"\"\n",
    "    data = pandas.read_csv(\"Data/%s.csv\"%filename, sep=',')\n",
    "    actions = list(np.unique(data.loc[:,\"a\"]))\n",
    "    if filename ==\"small\":\n",
    "        states = list(set(np.unique(data.loc[:,\"s\"])) | set(np.unique(data.loc[:,\"sp\"])))\n",
    "        discount = 0.95\n",
    "    if filename ==\"medium\":\n",
    "        states = list(set(np.unique(data.loc[:,\"s\"])) | set(np.unique(data.loc[:,\"sp\"])))\n",
    "        discount = 1\n",
    "    if filename ==\"large\":\n",
    "        states = list(set(np.unique(data.loc[:,\"s\"])) | set(np.unique(data.loc[:,\"sp\"])))\n",
    "        discount = 0.95\n",
    "    return data, states, actions, discount\n",
    "\n",
    "\n",
    "class MDP(object):\n",
    "    \"\"\"Object that encapsulate the MDP and its methods\n",
    "    \n",
    "    Attributes:\n",
    "        -states (list): List of states in the samples (subset of the full state space).\n",
    "        -actions (list): List of actions in the samples (subset of the full action space).\n",
    "        -discount (float): Discount factor.\n",
    "        -U (dataframe): Pandas dataframe that contains the U(s) values for s in states.\n",
    "        -Q (dataframe): Pandas dataframe that contains the Q(s,a) values for s in states and a in actions.\n",
    "        -N (dict): Dictionary that contains the counts of transitions N(s,a,s') values for s in states, a in actions and s' in next states.\n",
    "        -T (dict): Dictionary that contains the transition estimates T(s,a,s') values for s in states, a in actions and s' in next states.\n",
    "        -rau (dict): Dictionary that contains the sum of rewards rau(s,a) values for s in states and a in actions.\n",
    "        -R (dict): Dictionary that contains the reward estimates R(s,a) values for s in states and a in actions.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, states, actions, discount):\n",
    "        \n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.discount = discount\n",
    "        self.U = pandas.DataFrame(data=np.zeros((len(states),1)), index=states, dtype=float, copy=False)\n",
    "        self.Q = pandas.DataFrame(data=np.zeros((len(states),len(actions))), index=states, columns=actions, dtype=float, copy=False)\n",
    "        self.N, self.rau, self.R, self.T = {}, {}, {}, {}\n",
    "        for current_state in self.states:\n",
    "            for action in self.actions:\n",
    "                self.rau[(current_state, action)] = 0.0\n",
    "                self.R[(current_state, action)] = 0.0\n",
    "                self.N[(current_state, action)] = {}\n",
    "                self.T[(current_state, action)] = {}    \n",
    "        \n",
    "    def policy_evaluation(self, policy, n=20):\n",
    "        \"\"\"Method that implements Policy Evaluation\n",
    "    \n",
    "        Args:\n",
    "            -policy (dict): Policy for every states.\n",
    "            -n (int): Number of iterations.\n",
    "        \n",
    "        \"\"\"\n",
    "        for i in range(n):\n",
    "            for current_state in self.states:\n",
    "                self.update_U(current_state, policy=policy)\n",
    "                \n",
    "    def policy_iteration(self, data, max_iter=100):\n",
    "        \"\"\"Method that implements Policy iteration\n",
    "    \n",
    "        Args:\n",
    "            -data (dataframe): Dataset of the sampled transitions.\n",
    "            -max_iter (int): Maximum number of iterations.\n",
    "        \n",
    "        \"\"\"\n",
    "        for t in range(len(data)):\n",
    "            current_state, action, reward, next_state = extract_sample(data,t)\n",
    "            if self.N[(current_state, action)].has_key(next_state):\n",
    "                self.N[(current_state, action)][next_state] += 1.0\n",
    "            else:\n",
    "                self.N[(current_state, action)][next_state] = 1.0\n",
    "            self.rau[(current_state, action)] += reward\n",
    "        \n",
    "        for current_state in self.states:\n",
    "            for action in self.actions:\n",
    "                self.estimate_T_and_R(current_state, action)\n",
    "\n",
    "        policy = self.policy_from_values()\n",
    "        for i in range(max_iter):\n",
    "            if i%10==0:\n",
    "                print \"iteration: %d\"%i\n",
    "            old_policy = policy\n",
    "            self.policy_evaluation(policy, n=30)\n",
    "            policy = self.policy_from_values()\n",
    "            if np.mean(np.asarray(policy.values()) == np.asarray(old_policy.values()))==1.0:\n",
    "                print \"Policy convergence at iteration %d\"%i\n",
    "                break\n",
    "        return policy\n",
    "        \n",
    "    def value_iteration(self, data, max_iter=100):\n",
    "        \"\"\"Method that implements Value iteration\n",
    "    \n",
    "        Args:\n",
    "            -data (dataframe): Dataset of the sampled transitions.\n",
    "            -max_iter (int): Maximum number of iterations.\n",
    "        \n",
    "        \"\"\"\n",
    "        for t in range(len(data)):\n",
    "            current_state, action, reward, next_state = extract_sample(data,t)\n",
    "            if self.N[(current_state, action)].has_key(next_state):\n",
    "                self.N[(current_state, action)][next_state] += 1.0\n",
    "            else:\n",
    "                self.N[(current_state, action)][next_state] = 1.0\n",
    "            self.rau[(current_state, action)] += reward\n",
    "        \n",
    "        for current_state in self.states:\n",
    "            for action in self.actions:\n",
    "                self.estimate_T_and_R(current_state, action)\n",
    "\n",
    "        for i in range(max_iter):\n",
    "            if i%10==0:\n",
    "                print \"iteration: %d\"%i\n",
    "            old_U = self.U.values()\n",
    "            for current_state in self.states:\n",
    "                self.update_U(current_state)\n",
    "            if np.mean(np.asarray(self.U.values()) == np.asarray(old_U))==1.0:\n",
    "                print \"Convergence at iteration %d\"%i\n",
    "                break\n",
    "\n",
    "    def model_based(self, data):\n",
    "        \"\"\"Method that implements Model Based RL\n",
    "    \n",
    "        Args:\n",
    "            -data (dataframe): Dataset of the sampled transitions.\n",
    "\n",
    "        \"\"\"\n",
    "        for t in range(len(data)):\n",
    "            current_state, action, reward, next_state = extract_sample(data,t)\n",
    "            if self.N[(current_state, action)].has_key(next_state):\n",
    "                self.N[(current_state, action)][next_state] += 1.0\n",
    "            else:\n",
    "                self.N[(current_state, action)][next_state] = 1.0\n",
    "            self.rau[(current_state, action)] += reward\n",
    "            self.estimate_T_and_R(current_state, action)\n",
    "            self.update_Q(current_state, action)\n",
    "            \n",
    "                    \n",
    "    def q_learning(self, data, learning_rate=0.1, n_iter=1):\n",
    "        \"\"\"Method that implements Q learning\n",
    "    \n",
    "        Args:\n",
    "            -data (dataframe): Dataset of the sampled transitions.\n",
    "            -learning_rate (float): Learning rate for Qlearning.\n",
    "            -n_iter (int): Number of iterations.\n",
    "\n",
    "        \"\"\"\n",
    "        for i in range(n_iter):\n",
    "            for t in range(len(data)):\n",
    "                current_state, action, reward, next_state = extract_sample(data,t)\n",
    "                self.Q.loc[current_state, action] += learning_rate*(reward + self.discount*np.max([self.Q.loc[next_state, next_action] for next_action in self.actions]) - self.Q.loc[current_state, action])\n",
    "\n",
    "\n",
    "    def sarsa(self, data, learning_rate=0.1, n_iter=1):\n",
    "        \"\"\"Method that implements SARSA\n",
    "    \n",
    "        Args:\n",
    "            -data (dataframe): Dataset of the sampled transitions.\n",
    "            -learning_rate (float): Learning rate for Qlearning.\n",
    "            -n_iter (int): Number of iterations.\n",
    "\n",
    "        \"\"\"\n",
    "        for i in range(n_iter):\n",
    "            for t in range(len(data)-1):\n",
    "                if t%100000==0:\n",
    "                    print t, len(data)\n",
    "                current_state, action, reward, next_state = extract_sample(data,t)\n",
    "                _, next_action, _, _ = extract_sample(data,t+1)\n",
    "                self.Q.loc[current_state, action] += learning_rate*(reward + self.discount*self.Q.loc[next_state, next_action] - self.Q.loc[current_state, action])\n",
    "    \n",
    "    def q_learning_lambda(self, data, learning_rate=0.1, n_iter=1, lambda_coef=0.9):\n",
    "        \"\"\"Method that implements Qlearning(lambda)\n",
    "        \n",
    "        Args:\n",
    "            -data (dataframe): Dataset of the sampled transitions.\n",
    "            -learning_rate (float): Learning rate for Qlearning.\n",
    "            -n_iter (int): Number of iterations.\n",
    "            -lambda_coef (float): Lambda coefficient.\n",
    "\n",
    "        \"\"\"\n",
    "        for i in range(n_iter):\n",
    "            for t in range(len(data)):\n",
    "                current_state, action, reward, next_state = extract_sample(data,t)\n",
    "                self.N[(current_state,action)] += 1\n",
    "                delta = reward + self.discount*np.max([self.Q[(next_state, next_action)] for next_action in self.actions]) - self.Q[(current_state, action)]\n",
    "                for state in self.states:\n",
    "                    for action in self.actions:\n",
    "                        self.Q[(state, action)] += learning_rate*delta*self.N[(current_state,action)]\n",
    "                        self.N[(current_state,action)] = self.N[(current_state,action)]*lambda_coef*self.discount\n",
    "      \n",
    "    \n",
    "    def estimate_T_and_R(self, current_state, action):\n",
    "        \"\"\"Method to estimate T and R using maxiumum likelihood estimation.\n",
    "        \n",
    "        Args:\n",
    "            -current_state (int): Current state in the sampled transition.\n",
    "            -action (int): Current action in the sampled transition.\n",
    "\n",
    "        \"\"\"\n",
    "        N_total = np.sum(self.N[(current_state, action)].values())\n",
    "        if N_total != 0:\n",
    "            self.R[(current_state, action)] = self.rau[(current_state, action)]/N_total\n",
    "        for next_state in self.N[(current_state, action)].keys():\n",
    "            self.T[(current_state, action)][next_state] = self.N[(current_state, action)][next_state]/N_total\n",
    "\n",
    "    def update_Q(self, current_state, action):\n",
    "        \"\"\"Method that updates Q\n",
    "        \n",
    "        Args:\n",
    "            -current_state (int): Current state in the sampled transition.\n",
    "            -action (int): Current action in the sampled transition.\n",
    "\n",
    "        \"\"\"\n",
    "        res = 0\n",
    "        for next_state in self.T[(current_state, action)].keys():\n",
    "            res += self.T[(current_state, action)][next_state]*np.max([self.Q[(next_state, next_action)] for next_action in self.actions])\n",
    "        self.Q[(current_state, action)] = self.R[(current_state, action)] + self.discount*res\n",
    "        \n",
    "    def update_U(self, current_state, policy=None):\n",
    "        \"\"\"Method that updates U\n",
    "        \n",
    "        Args:\n",
    "            -current_state (int): Current state in the sampled transition.\n",
    "            -policy (dict): Optional policy to follow.\n",
    "\n",
    "        \"\"\"\n",
    "        if policy == None:\n",
    "            candidates = []\n",
    "            for action in self.actions:\n",
    "                res = np.sum([self.T[(current_state, action)][next_state]*self.U[next_state] for next_state in self.T[(current_state, action)].keys()])\n",
    "                candidates.append(self.R[(current_state, action)] + self.discount*res)\n",
    "            self.U[(current_state)] = np.max(candidates)\n",
    "        \n",
    "        else:\n",
    "            action = policy[current_state]\n",
    "            res = np.sum([self.T[(current_state, action)][next_state]*self.U[next_state] for next_state in self.T[(current_state, action)].keys()])\n",
    "            self.U[(current_state)] = self.R[(current_state, action)] + self.discount*res\n",
    "    \n",
    "    \n",
    "    def update_U_from_Q(self):\n",
    "        \"\"\"Method that updates U from Q\n",
    "\n",
    "        \"\"\"\n",
    "        for current_state in self.states:\n",
    "            self.U.loc[current_state,0] = np.max([self.Q.loc[current_state,action] for action in self.actions])\n",
    "\n",
    "\n",
    "    def policy_from_values(self):\n",
    "        \"\"\"Method that extract the policy from U\n",
    "        \n",
    "        Args:\n",
    "            -filename (str): Name of the dataset.\n",
    "        \n",
    "        Returns:\n",
    "            -policy (dict): The policy for every states.\n",
    "\n",
    "        \"\"\"\n",
    "        if filename == \"small\":\n",
    "            policy = {state:1 for state in range(1,101,1)}\n",
    "        if filename == \"medium\":\n",
    "            policy = {state:1 for state in range(1,50001,1)}\n",
    "        if filename == \"large\":\n",
    "            policy = {state:1 for state in range(1,10101011,1)}\n",
    "        for state in self.states:\n",
    "            policy[state] = self.actions[np.argmax([self.R[(state, action)] + self.discount*np.sum([self.T[(state, action)][next_state]*self.U[next_state] for next_state in self.T[(state, action)].keys()]) for action in self.actions])]\n",
    "        return policy\n",
    "    \n",
    "    \n",
    "    def policy_from_Qvalues(self, filename):\n",
    "        \"\"\"Method that extract the policy from Q\n",
    "        \n",
    "        Args:\n",
    "            -filename (str): Name of the dataset.\n",
    "        \n",
    "        Returns:\n",
    "            -policy (dict): The policy for every states.\n",
    "\n",
    "        \"\"\"\n",
    "        if filename == \"small\":\n",
    "            policy = {state:1 for state in range(1,101,1)}\n",
    "        if filename == \"medium\":\n",
    "            policy = {state:1 for state in range(1,50001,1)}\n",
    "        if filename == \"large\":\n",
    "            policy = {state:1 for state in range(1,10101011,1)}\n",
    "        for state in self.states:\n",
    "            if np.mean([self.Q.loc[state, action] for action in self.actions])==0:\n",
    "                policy[state] = self.actions[np.random.choice(len(self.actions))]\n",
    "            else:\n",
    "                policy[state] = self.actions[np.argmax([self.Q.loc[state, action] for action in self.actions])]\n",
    "        return policy\n",
    "    \n",
    "\n",
    "class ApproxMDP(object):\n",
    "    \"\"\"Object that encapsulate the approximate MDP and its methods\n",
    "    \n",
    "    Attributes:\n",
    "        -states (list): List of states in the samples (subset of the full state space).\n",
    "        -actions (list): List of actions in the samples (subset of the full action space).\n",
    "        -discount (float): Discount factor.\n",
    "        -nb_features (int): Number of features in the approximation\n",
    "        -thetas (np.array): Theta coefficients.\n",
    "        -U (dict): Dictionary that contains the U(s) values for s in states.\n",
    "        -Q (dict): Dictionary that contains the Q(s,a) values for s in states and a in actions.\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, states, actions, discount, card_x, card_y, nb_features):\n",
    "        \n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.discount = discount\n",
    "        self.nb_features = nb_features\n",
    "        self.thetas = np.zeros((self.nb_features*len(self.actions),1))\n",
    "        self.Q = {}\n",
    "        self.U = {}\n",
    "        for current_state in self.states:\n",
    "            self.U[current_state] = 0.0\n",
    "            for action in self.actions:\n",
    "                self.Q[(current_state, action)] = 0\n",
    "\n",
    "    def q_learning(self, data, learning_rate=0.1, n_iter=1):\n",
    "        \"\"\"Method that implements approximate Q learning\n",
    "    \n",
    "        Args:\n",
    "            -data (dataframe): Dataset of the sampled transitions.\n",
    "            -learning_rate (float): Learning rate for Qlearning.\n",
    "            -n_iter (int): Number of iterations.\n",
    "\n",
    "        \"\"\"\n",
    "        for i in range(n_iter):\n",
    "            for t in range(len(data)):\n",
    "                current_state, action, reward, next_state = extract_sample(data,t)\n",
    "                self.thetas += learning_rate*(reward + self.discount*np.max([self.thetas.T.dot(self.get_features(next_state, next_action))[0,0] for next_action in self.actions]) - self.thetas.T.dot(self.get_features(current_state, action))[0,0])*self.get_features(current_state, action)\n",
    "        for current_state in self.states:\n",
    "            for action in self.actions:\n",
    "                self.Q[(current_state, action)] = self.thetas.T.dot(self.get_features(current_state, action))[0,0]\n",
    "        \n",
    "        \n",
    "    def get_features(self, state, action):\n",
    "        \"\"\"Method that extract the features from a state s\n",
    "    \n",
    "        Args:\n",
    "            -state: Current state.\n",
    "            -action: Current action.\n",
    "\n",
    "        \"\"\"\n",
    "        features = np.zeros((self.nb_features*len(self.actions),1))\n",
    "        x = float((state-1)/10)/10\n",
    "        y = float((state-1)%10)/10\n",
    "        features[self.nb_features*list(self.actions).index(action) + 0] = 1.0\n",
    "        features[self.nb_features*list(self.actions).index(action) + 1] = x\n",
    "        features[self.nb_features*list(self.actions).index(action) + 2] = y\n",
    "        features[self.nb_features*list(self.actions).index(action) + 3] = x**2\n",
    "        features[self.nb_features*list(self.actions).index(action) + 4] = y**2\n",
    "        features[self.nb_features*list(self.actions).index(action) + 5] = x*y\n",
    "        \n",
    "        return features\n",
    "\n",
    "    def update_U_from_Q(self):\n",
    "        \"\"\"Method that updates U from Q\n",
    "\n",
    "        \"\"\"\n",
    "        for current_state in self.states:\n",
    "            self.U[current_state] = np.max([self.Q[(current_state,action)] for action in self.actions])\n",
    "\n",
    "    def policy_from_Qvalues(self):\n",
    "        \"\"\"Method that extract the policy from Q\n",
    "        \n",
    "        Args:\n",
    "            -filename (str): Name of the dataset.\n",
    "        \n",
    "        Returns:\n",
    "            -policy (dict): The policy for every states.\n",
    "\n",
    "        \"\"\"\n",
    "        if filename == \"small\":\n",
    "            policy = {state:1 for state in range(1,101,1)}\n",
    "        if filename == \"medium\":\n",
    "            policy = {state:1 for state in range(1,50001,1)}\n",
    "        if filename == \"large\":\n",
    "            policy = {state:1 for state in range(1,10101011,1)}\n",
    "        for state in self.states:\n",
    "            policy[state] = self.actions[np.argmax([self.Q[(state, action)] for action in self.actions])]\n",
    "        return policy\n",
    "\n",
    "def extract_sample(data,t):\n",
    "    \"\"\"Function to extract the transition at time t from the dataset\n",
    "    \n",
    "    Args:\n",
    "        -data (dataframe): Dataset of the sampled transitions.\n",
    "        -t (int): Time of the transition.\n",
    "    \n",
    "    Return:\n",
    "        -current_state\n",
    "        -action\n",
    "        -reward\n",
    "        -next_state\n",
    "    \n",
    "    \"\"\"\n",
    "    current_state = data.loc[t,'s']\n",
    "    action = data.loc[t,'a']\n",
    "    reward = data.loc[t,'r']\n",
    "    next_state = data.loc[t,'sp']\n",
    "    return current_state, action, reward, next_state\n",
    "\n",
    "def draw_policy_values(policy, U, card_x, card_y):\n",
    "    \"\"\"Function to draw a policy\n",
    "    \n",
    "    Args:\n",
    "        -policy (dict): The policy for every states.\n",
    "        -U (np.array): The Values stored in a np.array\n",
    "\n",
    "    \"\"\"\n",
    "    policy_map = np.zeros((card_x,card_y))\n",
    "    value_map = np.zeros((card_x,card_y))\n",
    "    for x in range(card_x):\n",
    "        for y in range(card_y):\n",
    "            policy_map[x,y] = policy[card_y*x+y+1]\n",
    "            value_map[x,y] = U[card_y*x+y]\n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(value_map, cmap='hot', interpolation='nearest')\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(policy_map, cmap='hot', interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "def save_policy(policy, filename):\n",
    "    \"\"\"Function to save the policy\n",
    "    \n",
    "    Args:\n",
    "        -policy (dict): The policy for every states.\n",
    "        -filename (str): Location of the file to save the policy.\n",
    "    \n",
    "    \"\"\"\n",
    "    print \"Saving the policy as %s.policy\"%filename\n",
    "    with open(\"Data/%s.policy\"%filename, 'w') as f:\n",
    "        for action in policy.values():\n",
    "            f.write(\"%d\\n\"%action)\n",
    "\n",
    "\n",
    "def compute(filename):\n",
    "    \"\"\"Function that wraps all computations\n",
    "    \n",
    "    Args:\n",
    "        -filename (str): Name of the dataset.\n",
    "    \n",
    "    \"\"\"\n",
    "    data, states, actions, discount = load_data(filename)\n",
    "    mdp = MDP(states, actions, discount)\n",
    "    mdp.sarsa(data, learning_rate=0.1, n_iter=10)\n",
    "    policy = mdp.policy_from_Qvalues(filename)\n",
    "    save_policy(policy, filename)\n",
    "\n",
    "\n",
    "def main():\n",
    "    if len(sys.argv) != 2:\n",
    "        raise Exception(\"usage: python project2.py filename\")\n",
    "    \n",
    "    filename = sys.argv[1]\n",
    "    compute(filename)\n",
    "\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "#    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data, states, actions, discount = load_data('large')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s        1817\n",
       "a          92\n",
       "r           0\n",
       "sp    1002827\n",
       "Name: 366642, dtype: int64"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[366642,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data, states, actions, discount = load_data('large')\n",
    "mdp = MDP(states, actions, discount)\n",
    "mdp.q_learning(data, learning_rate=0.1, n_iter=10)\n",
    "mdp.update_U_from_Q()\n",
    "policy = mdp.policy_from_Qvalues('medium')\n",
    "#save_policy(policy, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2QAAAGfCAYAAAAwIhE7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrp23UgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f5311d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_policy_values(policy,mdp.U.values,10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
